{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-A9fBYTdmQF"
      },
      "outputs": [],
      "source": [
        "1. Core Components of the Hadoop Ecosystem\n",
        "HDFS:\n",
        "The Hadoop Distributed File System is designed for storing large datasets across multiple nodes in a cluster. It splits files into blocks and distributes them across nodes, ensuring fault tolerance and high throughput.\n",
        "\n",
        "MapReduce:\n",
        "MapReduce is a programming model for processing large datasets in parallel. It splits data processing into two phases:\n",
        "\n",
        "Map Phase: Processes input data and produces key-value pairs.\n",
        "Reduce Phase: Aggregates or reduces the key-value pairs into meaningful output.\n",
        "YARN (Yet Another Resource Negotiator):\n",
        "YARN is Hadoop's cluster resource management layer. It allocates resources to various applications and schedules tasks efficiently."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ". Detailed Explanation of HDFS\n",
        "NameNode: The master node, which manages metadata (file locations, blocks, etc.). It doesn't store the actual data.\n",
        "DataNodes: The worker nodes where actual data blocks are stored.\n",
        "Blocks: Files are split into fixed-size blocks (default 128MB) and stored across DataNodes.\n",
        "Key Features:\n",
        "Fault Tolerance: Data is replicated (default 3 copies) across nodes.\n",
        "Scalability: New nodes can be added seamlessly.\n",
        "Reliability: Data recovery is ensured even if nodes fail.\n"
      ],
      "metadata": {
        "id": "t5T-R5WAdzxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. MapReduce Framework\n",
        "How It Works:\n",
        "Input Splitting: Data is split into chunks.\n",
        "Mapping: Each chunk is processed to produce intermediate key-value pairs.\n",
        "Shuffling: Intermediate pairs are grouped by keys and sent to reducers.\n",
        "Reducing: Aggregates the grouped data to produce the final output.\n",
        "Example:\n",
        "Counting word frequency in a book:\n",
        "\n",
        "Map: Split text into words and emit each word as (word, 1).\n",
        "Reduce: Sum the values for each word key.\n",
        "Advantages:\n",
        "Handles large-scale data.\n",
        "Fault-tolerant.\n",
        "Limitations:\n",
        "High disk I/O due to intermediate data storage.\n",
        "Limited real-time processing capabilities."
      ],
      "metadata": {
        "id": "2KoxNEMPdz5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. Role of YARN\n",
        "Cluster Management:\n",
        "YARN separates resource management from task scheduling, allowing multiple applications to run concurrently.\n",
        "\n",
        "ResourceManager: Allocates cluster resources.\n",
        "NodeManager: Manages node-specific tasks.\n",
        "Comparison with Hadoop 1.x:\n",
        "Hadoop 1.x tied MapReduce tightly with resource management, limiting flexibility.\n",
        "YARN decouples them, enabling broader ecosystem integration.\n",
        "Benefits:\n",
        "Enhanced scalability.\n",
        "Supports multiple frameworks.\n"
      ],
      "metadata": {
        "id": "UVXQBf9vd0CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ntJOFZucd0R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "5. Popular Hadoop Ecosystem Components\n",
        "Components:\n",
        "HBase: NoSQL database for real-time reads/writes.\n",
        "Hive: SQL-like querying for data stored in HDFS.\n",
        "Pig: High-level scripting for data analysis.\n",
        "Spark: Fast, in-memory data processing.\n",
        "Example Use Case:\n",
        "Using Hive for querying log data stored in HDFS for business insights."
      ],
      "metadata": {
        "id": "gH7iWx6JeMk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Spark vs. MapReduce\n",
        "Key Differences:\n",
        "Spark processes data in-memory, reducing disk I/O.\n",
        "Spark supports iterative and real-time processing.\n",
        "Spark has APIs for diverse tasks (e.g., SQL, machine learning).\n",
        "How Spark Overcomes Limitations:\n",
        "Faster performance via in-memory computation.\n",
        "User-friendly APIs for complex workflows."
      ],
      "metadata": {
        "id": "NVFHOsRVeMoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. Spark Word Count Application\n",
        "Code (Python):\n",
        "python\n",
        "Copy code\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"Word Count\")\n",
        "text_file = sc.textFile(\"input.txt\")\n",
        "\n",
        "word_counts = (text_file\n",
        "               .flatMap(lambda line: line.split())\n",
        "               .map(lambda word: (word, 1))\n",
        "               .reduceByKey(lambda a, b: a + b)\n",
        "               .sortBy(lambda x: x[1], ascending=False))\n",
        "\n",
        "top_10 = word_counts.take(10)\n",
        "print(top_10)\n",
        "Key Steps:\n",
        "Read input.\n",
        "Split text into words.\n",
        "Count occurrences.\n",
        "Sort and retrieve top 10 words.\n",
        "8. Spark RDD Tasks\n",
        "a. Filter Data:\n",
        "python\n",
        "Copy code\n",
        "filtered_data = rdd.filter(lambda x: x['age'] > 30)\n",
        "b. Map Transformation:\n",
        "python\n",
        "Copy code\n",
        "transformed_data = rdd.map(lambda x: (x['name'], x['age'] * 2))\n",
        "c. Reduce:\n",
        "python\n",
        "Copy code\n",
        "total_age = rdd.map(lambda x: x['age']).reduce(lambda a, b: a + b)\n",
        "9. Spark DataFrame Operations\n",
        "a. Select Columns:\n",
        "python\n",
        "Copy code\n",
        "df.select(\"name\", \"age\")\n",
        "b. Filter Rows:\n",
        "python\n",
        "Copy code\n",
        "df.filter(df['age'] > 30)\n",
        "c. Group and Aggregate:\n",
        "python\n",
        "Copy code\n",
        "df.groupBy(\"department\").agg({\"salary\": \"avg\"})\n",
        "d. Join DataFrames:\n",
        "python\n",
        "Copy code\n",
        "df1.join(df2, df1[\"id\"] == df2[\"id\"])\n",
        "10. Spark Streaming Application\n",
        "Steps:\n",
        "Ingest data from Kafka.\n",
        "Apply transformations like filtering.\n",
        "Write results to a database.\n",
        "11. Apache Kafka\n",
        "Kafka is a distributed messaging system for real-time data streaming. It solves challenges like high-throughput data ingestion and fault tolerance.\n",
        "\n",
        "12. Kafka Architecture\n",
        "Producers: Send data to topics.\n",
        "Topics: Logical categories for data.\n",
        "Brokers: Distribute data across the cluster.\n",
        "Consumers: Read data from topics.\n",
        "ZooKeeper: Coordinates cluster state.\n",
        "13. Kafka Guide\n",
        "Produce Data:\n",
        "python\n",
        "Copy code\n",
        "from kafka import KafkaProducer\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
        "producer.send('topic_name', b'message')\n",
        "producer.close()\n",
        "Consume Data:\n",
        "python\n",
        "Copy code\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "consumer = KafkaConsumer('topic_name', bootstrap_servers='localhost:9092')\n",
        "for message in consumer:\n",
        "    print(message.value)\n",
        "14. Kafka Retention and Partitioning\n",
        "Retention: Defines how long data is stored.\n",
        "Partitioning: Distributes load across brokers.\n",
        "15. Kafka Use Cases\n",
        "Example:\n",
        "Netflix uses Kafka for real-time analytics and log aggregation, enabling scalable, fault-tolerant processing.\n",
        "\n",
        "Let me know if you need further clarification or specific examples!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FIEr_BWseRrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZ1OUTRSeRvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YKfLb8yeRzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlBHG4mWeR2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}